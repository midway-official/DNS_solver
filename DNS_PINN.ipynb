{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1vJk_075zhndEMT5Dqg3Sjg2aS7NNfz3i",
      "authorship_tag": "ABX9TyMXWXb3kDpaMBLwfDX3FvNx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/midway-official/DNS_solver/blob/main/DNS_PINN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8Ts2ZoEN_J5",
        "outputId": "b08819d7-7121-4904-eea3-f97e9d21c791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install  tensorflow\n",
        "!pip install  numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm  # 导入tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ],
      "metadata": {
        "id": "96J_4hclOyzc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_data(folder):\n",
        "    \"\"\"\n",
        "    从指定文件夹加载 u, v, p, x, y 的数据。\n",
        "    \"\"\"\n",
        "    u = np.loadtxt(os.path.join(folder, \"u.dat\"))\n",
        "    v = np.loadtxt(os.path.join(folder, \"v.dat\"))\n",
        "    p = np.loadtxt(os.path.join(folder, \"p.dat\"))\n",
        "    x = np.loadtxt(os.path.join(folder, \"x.dat\"))\n",
        "    y = np.loadtxt(os.path.join(folder, \"y.dat\"))\n",
        "    return u, v, p, x, y\n",
        "\n",
        "def process_data(u, v, p, x, y):\n",
        "    \"\"\"\n",
        "    将 u, v, p 矩阵与坐标 (x, y) 对应，生成 PINN 的输入数据 (x, y, t) 和输出 (u, v, p)。\n",
        "    \"\"\"\n",
        "    rows, cols = u.shape\n",
        "    xyz_train = []\n",
        "    uvp_train = []\n",
        "\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            # 矩阵中的 (i, j) 转换为坐标 (x[j], y[-(i+1)])\n",
        "            x_coord = x[j]\n",
        "            y_coord = y[-(i+1)]  # y 倒序索引\n",
        "            xyz_train.append([x_coord, y_coord])  # 加入时间维度时可扩展\n",
        "            uvp_train.append([u[i, j], v[i, j], p[i, j]])\n",
        "\n",
        "    return np.array(xyz_train), np.array(uvp_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "wWewq0nfPVJn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def load_multiple_timesteps(base_folder, timesteps):\n",
        "\n",
        "    xyz_train_all = []\n",
        "    uvp_train_all = []\n",
        "    dt = 0.01\n",
        "    for t in timesteps:\n",
        "        print(f\"Loading: t = {t}\")\n",
        "        folder = os.path.join(base_folder, str(t))  # 时间步文件夹\n",
        "        u, v, p, x, y = load_data(folder)\n",
        "        xyz_train, uvp_train = process_data(u, v, p, x, y)\n",
        "\n",
        "        # 为每个点添加时间信息 t\n",
        "        t_column = np.full((xyz_train.shape[0], 1), dt*t)\n",
        "        xyz_train = np.hstack([xyz_train, t_column])\n",
        "\n",
        "        # 合并所有时间步的数据\n",
        "        xyz_train_all.append(xyz_train)\n",
        "        uvp_train_all.append(uvp_train)\n",
        "\n",
        "    return np.vstack(xyz_train_all), np.vstack(uvp_train_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NTIm2rh_Pl8A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "869c08d6-ed06-4ce8-f65b-78990f2ff320"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef load_multiple_timesteps(base_folder, timesteps):\\n\\n    xyz_train_all = []\\n    uvp_train_all = []\\n    dt = 0.01\\n    for t in timesteps:\\n        print(f\"Loading: t = {t}\")\\n        folder = os.path.join(base_folder, str(t))  # 时间步文件夹\\n        u, v, p, x, y = load_data(folder)\\n        xyz_train, uvp_train = process_data(u, v, p, x, y)\\n\\n        # 为每个点添加时间信息 t\\n        t_column = np.full((xyz_train.shape[0], 1), dt*t)\\n        xyz_train = np.hstack([xyz_train, t_column])\\n\\n        # 合并所有时间步的数据\\n        xyz_train_all.append(xyz_train)\\n        uvp_train_all.append(uvp_train)\\n\\n    return np.vstack(xyz_train_all), np.vstack(uvp_train_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_multiple_timesteps(base_folder, timesteps):\n",
        "    \"\"\"\n",
        "    高效加载多个时间步的数据，并添加时间信息。\n",
        "    使用并行处理加速数据加载，并显示进度条。\n",
        "    \"\"\"\n",
        "    dt = 0.01\n",
        "    xyz_train_all = []\n",
        "    uvp_train_all = []\n",
        "\n",
        "    def load_and_process_timestep(t):\n",
        "        \"\"\"\n",
        "        加载和处理单个时间步的数据\n",
        "        \"\"\"\n",
        "        folder = os.path.join(base_folder, str(t))  # 时间步文件夹\n",
        "        u, v, p, x, y = load_data(folder)\n",
        "        xyz_train, uvp_train = process_data(u, v, p, x, y)\n",
        "\n",
        "        # 为每个点添加时间信息 t\n",
        "        t_column = np.full((xyz_train.shape[0], 1), dt*t)\n",
        "        xyz_train = np.hstack([xyz_train, t_column])\n",
        "\n",
        "        return xyz_train, uvp_train\n",
        "\n",
        "    # 使用 ThreadPoolExecutor 来并行处理多个时间步，并显示进度条\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        # 使用 tqdm 显示进度条\n",
        "        results = list(tqdm(executor.map(load_and_process_timestep, timesteps), total=len(timesteps)))\n",
        "\n",
        "    # 将所有时间步的数据合并\n",
        "    for xyz_train, uvp_train in results:\n",
        "        xyz_train_all.append(xyz_train)\n",
        "        uvp_train_all.append(uvp_train)\n",
        "\n",
        "    # 合并所有时间步的数据\n",
        "    return np.vstack(xyz_train_all), np.vstack(uvp_train_all)"
      ],
      "metadata": {
        "id": "DcXaip1ElkDR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载时间步 0 到 800 的数据\n",
        "result_folder = '/content/drive/MyDrive/result30'  # 结果文件夹路径\n",
        "timesteps = range(0, 300)  # 200 个时间步\n",
        "\n",
        "# 加载数据\n",
        "xyz_data_all, uvp_data_all = load_multiple_timesteps(result_folder, timesteps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7g2qJg7QSFn",
        "outputId": "07fb65a3-7132-4a55-b943-363f35a38a3e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [00:07<00:00, 40.84it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#打印检查\n",
        "print(\"xyz_data_all shape:\", xyz_data_all.shape)  # 打印输入的形状\n",
        "print(\"uvp_data_all shape:\", uvp_data_all.shape)  # 打印输出的形状"
      ],
      "metadata": {
        "id": "3q_VbNwOQcZX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3adeabfa-d035-45d3-cd48-238fd23dc2f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xyz_data_all shape: (307200, 3)\n",
            "uvp_data_all shape: (307200, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存为 .npy 文件\n",
        "np.save('xyz_data_all.npy', xyz_data_all)\n",
        "np.save('uvp_data_all.npy', uvp_data_all)"
      ],
      "metadata": {
        "id": "IUI6Yd7wfdqJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义 PINN 模型\n",
        "class PINN(tf.keras.Model):\n",
        "    def __init__(self, layers):\n",
        "        super(PINN, self).__init__()\n",
        "        self.hidden_layers = []\n",
        "        for width in layers[1:-1]:\n",
        "            self.hidden_layers.append(tf.keras.layers.Dense(width, activation='tanh'))\n",
        "        self.output_layer = tf.keras.layers.Dense(layers[-1])\n",
        "\n",
        "    def call(self, x):\n",
        "        for layer in self.hidden_layers:\n",
        "            x = layer(x)\n",
        "        return self.output_layer(x)\n"
      ],
      "metadata": {
        "id": "D0FQAcM7Qf8b"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 构建 PINN 模型\n",
        "layers = [3,64,64,64,64,64,64,64, 3]  # 输入3个（x, y, t），输出3个（u, v, p）\n",
        "nu = 1/10000  # 粘性系数\n",
        "pinn = PINN(layers)"
      ],
      "metadata": {
        "id": "n4wIjkhVRLMD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 计算瞬态 Navier-Stokes 方程的损失\n",
        "def compute_loss_with_data(xyz_train, uvp_train, pinn, nu):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        tape.watch([xyz_train])\n",
        "\n",
        "        # 预测 u, v, p\n",
        "        outputs = pinn(xyz_train)\n",
        "        u_pred, v_pred, p_pred = outputs[:, 0:1], outputs[:, 1:2], outputs[:, 2:3]\n",
        "\n",
        "        # 数据驱动的损失\n",
        "        data_loss = tf.reduce_mean(tf.square(tf.cast(u_pred, tf.float32) - tf.cast(uvp_train[:, 0:1], tf.float32))) + \\\n",
        "            tf.reduce_mean(tf.square(tf.cast(v_pred, tf.float32) - tf.cast(uvp_train[:, 1:2], tf.float32))) + \\\n",
        "            tf.reduce_mean(tf.square(tf.cast(p_pred, tf.float32) - tf.cast(uvp_train[:, 2:3], tf.float32)))\n",
        "\n",
        "        # 计算一阶导数\n",
        "        u_x = tape.gradient(u_pred, xyz_train)[:, 0:1]\n",
        "        u_y = tape.gradient(u_pred, xyz_train)[:, 1:2]\n",
        "        u_t = tape.gradient(u_pred, xyz_train)[:, 2:3]\n",
        "        v_x = tape.gradient(v_pred, xyz_train)[:, 0:1]\n",
        "        v_y = tape.gradient(v_pred, xyz_train)[:, 1:2]\n",
        "        v_t = tape.gradient(v_pred, xyz_train)[:, 2:3]\n",
        "        p_x = tape.gradient(p_pred, xyz_train)[:, 0:1]\n",
        "        p_y = tape.gradient(p_pred, xyz_train)[:, 1:2]\n",
        "\n",
        "    # 计算二阶导数\n",
        "    u_xx = tape.gradient(u_x, xyz_train)[:, 0:1]\n",
        "    u_yy = tape.gradient(u_y, xyz_train)[:, 1:2]\n",
        "    v_xx = tape.gradient(v_x, xyz_train)[:, 0:1]\n",
        "    v_yy = tape.gradient(v_y, xyz_train)[:, 1:2]\n",
        "\n",
        "    del tape  # 删除梯度带释放资源\n",
        "\n",
        "    # Navier-Stokes 方程的残差项\n",
        "    # 动量方程（x方向）\n",
        "    f_u = tf.cast(u_t, tf.float32) + tf.cast(u_pred, tf.float32) * tf.cast(u_x, tf.float32) + \\\n",
        "          tf.cast(v_pred, tf.float32) * tf.cast(u_y, tf.float32) + tf.cast(p_x, tf.float32) - \\\n",
        "          tf.cast(nu, tf.float32) * (tf.cast(u_xx, tf.float32) + tf.cast(u_yy, tf.float32))\n",
        "\n",
        "    # 动量方程（y方向）\n",
        "    f_v = tf.cast(v_t, tf.float32) + tf.cast(u_pred, tf.float32) * tf.cast(v_x, tf.float32) + \\\n",
        "          tf.cast(v_pred, tf.float32) * tf.cast(v_y, tf.float32) + tf.cast(p_y, tf.float32) - \\\n",
        "          tf.cast(nu, tf.float32) * (tf.cast(v_xx, tf.float32) + tf.cast(v_yy, tf.float32))\n",
        "\n",
        "    # 连续性方程\n",
        "    f_continuity = tf.cast(u_x, tf.float32) + tf.cast(v_y, tf.float32)\n",
        "\n",
        "    # 计算 Navier-Stokes 方程残差损失\n",
        "    ns_loss = tf.reduce_mean(tf.square(f_u)) + tf.reduce_mean(tf.square(f_v)) + tf.reduce_mean(tf.square(f_continuity))\n",
        "\n",
        "    # 总损失\n",
        "    total_loss = data_loss + ns_loss\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "0W6L34VJRQwS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 定义优化器和训练过程\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "@tf.function\n",
        "def train_step(xyz_train, uvp_train, pinn, optimizer, nu):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = compute_loss_with_data(xyz_train, uvp_train, pinn, nu)\n",
        "    gradients = tape.gradient(loss, pinn.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, pinn.trainable_variables))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "1-SVPw4lRWdF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def split_data(xyz_data_all, uvp_data_all, train_ratio=0.8, val_ratio=0.1):\n",
        "    \"\"\"\n",
        "    将数据集划分为训练集、验证集和测试集。\n",
        "\n",
        "    参数:\n",
        "        xyz_data_all (np.ndarray): 包含所有坐标和时间数据\n",
        "        uvp_data_all (np.ndarray): 包含所有速度和压力数据\n",
        "        train_ratio (float): 训练集的比例\n",
        "        val_ratio (float): 验证集的比例\n",
        "\n",
        "    返回:\n",
        "        (train, validation, test): 训练集、验证集、测试集的元组\n",
        "    \"\"\"\n",
        "    total_size = xyz_data_all.shape[0]  # 使用 xyz_data_all，而不是 xyz_data\n",
        "    indices = np.arange(total_size)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    train_size = int(total_size * train_ratio)\n",
        "    val_size = int(total_size * val_ratio)\n",
        "    test_size = total_size - train_size - val_size\n",
        "\n",
        "    # 切分数据\n",
        "    train_indices = indices[:train_size]\n",
        "    val_indices = indices[train_size:train_size + val_size]\n",
        "    test_indices = indices[train_size + val_size:]\n",
        "\n",
        "    xyz_train = xyz_data_all[train_indices]\n",
        "    uvp_train = uvp_data_all[train_indices]\n",
        "\n",
        "    xyz_val = xyz_data_all[val_indices]\n",
        "    uvp_val = uvp_data_all[val_indices]\n",
        "\n",
        "    xyz_test = xyz_data_all[test_indices]\n",
        "    uvp_test = uvp_data_all[test_indices]\n",
        "\n",
        "    return (xyz_train, uvp_train), (xyz_val, uvp_val), (xyz_test, uvp_test)"
      ],
      "metadata": {
        "id": "viF3g_n9Rko6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 数据加载\n",
        "(xyz_train, uvp_train), (xyz_val, uvp_val), (xyz_test, uvp_test) = split_data(xyz_data_all, uvp_data_all)\n",
        "\n",
        "print(f\"Training data shape: {xyz_train.shape}, {uvp_train.shape}\")\n",
        "print(f\"Validation data shape: {xyz_val.shape}, {uvp_val.shape}\")\n",
        "print(f\"Test data shape: {xyz_test.shape}, {uvp_test.shape}\")\n"
      ],
      "metadata": {
        "id": "-ORaxg-wS81d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f566a79c-c289-49b8-ccda-f6c7a90d4f94"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (245760, 3), (245760, 3)\n",
            "Validation data shape: (30720, 3), (30720, 3)\n",
            "Test data shape: (30720, 3), (30720, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#转化为单精度张量\n",
        "xyz_val = tf.convert_to_tensor(xyz_val, dtype=tf.float32)\n",
        "uvp_val = tf.convert_to_tensor(uvp_val, dtype=tf.float32)\n",
        "epochs = 200\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(epochs), desc=\"Training Progress\", ncols=100):\n",
        "    # 训练\n",
        "    loss = train_step(xyz_train, uvp_train, pinn, optimizer, nu)\n",
        "\n",
        "    # 每 50 个 epoch 打印训练损失和验证损失\n",
        "    if epoch % 50 == 0:\n",
        "        # 验证\n",
        "        val_loss = tf.reduce_mean(compute_loss_with_data(xyz_val, uvp_val, pinn, nu))  # 验证集损失\n",
        "        print(f\"Epoch {epoch}/{epochs}, Training Loss: {loss.numpy():.6f}, Validation Loss: {val_loss.numpy():.6f}\")"
      ],
      "metadata": {
        "id": "pKSqhUlpTEqw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "9df2b229-00a6-4930-ca83-398e1904ffba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-edbf09a6e373>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#转化为单精度张量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxyz_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxyz_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0muvp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muvp_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 保存最终模型\n",
        "pinn.save(\"pinn_mode.h5\")"
      ],
      "metadata": {
        "id": "h5Orpz7AkvBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算测试集损失\n",
        "xyz_test = tf.convert_to_tensor(xyz_test, dtype=tf.float32)\n",
        "uvp_test = tf.convert_to_tensor(uvp_test, dtype=tf.float32)\n",
        "test_loss = tf.reduce_mean(compute_loss_with_data(xyz_test, uvp_test, pinn, nu))  # test集损失\n",
        "print(f\"Test Loss: {test_loss.numpy()}\")"
      ],
      "metadata": {
        "id": "9dnrXDPWTG3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 定义计算网格\n",
        "x_range = np.linspace(0, 1, 100)  # x方向\n",
        "y_range = np.linspace(0, 1, 100)  # y方向\n",
        "X, Y = np.meshgrid(x_range, y_range)\n",
        "\n",
        "# 定义时刻 t\n",
        "t = 2  # 假设是 t=0.05 的时刻\n",
        "\n",
        "# 将网格点打包为输入数据\n",
        "input_data = np.hstack([X.flatten()[:, None], Y.flatten()[:, None], np.full((X.size, 1), t)])\n",
        "\n",
        "# 使用 PINN 预测速度和压力\n",
        "pinn_output = pinn.predict(input_data)\n",
        "u = pinn_output[:, 0].reshape(X.shape)  # 提取 u 速度分量\n",
        "v = pinn_output[:, 1].reshape(X.shape)  # 提取 v 速度分量\n",
        "p = pinn_output[:, 2].reshape(X.shape)  # 提取压力\n",
        "\n",
        "# 归一化速度\n",
        "magnitude = np.sqrt(u**2 + v**2)\n",
        "u_normalized = u / magnitude\n",
        "v_normalized = v / magnitude"
      ],
      "metadata": {
        "id": "M3VZBgUfTgtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 绘制图像\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# 绘制速度大小的等高线图\n",
        "plt.subplot(1, 3, 1)\n",
        "contour = plt.contourf(X, Y, magnitude, levels=20, cmap='viridis')\n",
        "plt.colorbar(contour)\n",
        "plt.title('Velocity Magnitude')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "\n",
        "# 绘制归一化的速度矢量图，叠加压力等高线\n",
        "plt.subplot(1, 3, 2)\n",
        "contour_p = plt.contourf(X, Y, p, levels=20, cmap='autumn', alpha=0.6)\n",
        "plt.colorbar(contour_p)\n",
        "plt.title('Normalized Velocity and Pressure')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "\n",
        "# 绘制流线图\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.streamplot(X, Y, u, v, color='b', linewidth=0.3, density=3)\n",
        "plt.title('Streamlines')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QwldOLUQTlaN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}